# -*- coding: utf-8 -*-
"""NLP_DIV_B.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OgzbLp4LBSZqM5hJQZqRUt3V3lfrZiy2
"""

# gettin gstarted with NLTK
#-----------------------------------------------
import nltk  # natural language tool kit

# it import :
# the 9 sample texts (text1 to text9)
# word list
# stopwords
# some helper corpora

# --------------------------------------------
nltk.download('book')

# import all from book
#  ------------------------------

from nltk.book import *

texts()
sents()

print(text1)
print(text2)
print(text3)
print(text4)
print(text5)
print(text6)
print(text7)
print(text8)
print(text9)

print(sent1)
print(sent2)
print(sent3)
print(sent4)
print(sent5)
print(sent6)
print(sent7)
print(sent8)
print(sent9)

(list(text1))

# print(text1[0])
# print(text1[20])
# print(text1[40])

# stemming
# running > run
# studies > studi
# better > better
# happiness > happi

# lematization
# running > run
# studies > study
# better > better
# happiness > happy

from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.corpus import wordnet
from nltk.book import *


ps = PorterStemmer()
lz = WordNetLemmatizer()
# class say object banaya and then we we do llike ps.method()...

word = 'running'
print('stemming',ps.stem(word))
print('lematization',lz.lemmatize(word, pos='v'))

# stemming run
# lematization run

word = 'studying'
print('stemming',ps.stem(word))
print('lematization',lz.lemmatize(word, pos='v'))

# stemming studi
# lematization study

# inshot
# stemming > mechanical shortening
# lemmatization > linguistically correct base form

# searching texts
# here it gives similar words

text1.similar('monstrous')

#it gives you the word that fits in the same context as of the defined word 'monsterious'

#for example

# context1
# a monstrous whale
# a _ whale
# ----------------------
# a big whale
# small whale
# a beautiful whale

# -> so big , small , beautiful are similar words like - monstrous.
# so its basically giving simliar CONTEXT word.

# context2
# the monstrous gorilla
# the _ gorilla
# ---------------------
# the sweet gorilla
# the huge gorilla
# the african gorilla

text2.similar('monstrous')

# here it gives Context of a word or words.

#---------------------------------------

text2.common_contexts(['very'])

# am very pretty
# am much pretty
# am very glad
# am much glad
# be very glad
### so "am - pretty" ... yeah sab context hai.

# a_good be_glad a_short was_well is_true a_few know_well a_respectable
# are_much wished_much am_sure so_much a_comfortable was_much am_much
# the_first know_little a_fine is_pretty a_pretty
# yeah sab context hai jaha par very use kar sakte hai .

print ("done")

# -----------------------------------------

text2.common_contexts(['monstrous'])

# am monstrous pretty
# am monstrous pretty
# am monstrous glad
# be monstrous glad

# am_glad a_pretty a_lucky is_pretty be_glad is_fond was_happy a_deal
# yeah sab context hai jaha par monstrous use kar sakte hai .

print("done")

# -----------------------------------------

text2.common_contexts(['monstrous','very'])

# am_glad a_pretty a_lucky is_pretty be_glad
# here it gives the COMMON context for these words (so montstrous and very dono kay COMMON wale context dega).

text4.dispersion_plot(['citizens','democracy','freedom','duties','America'])

# couting vocabulary
text3.generate

print(len(text3))
print(len(set(text3)))

sorted(set(text3))

# counting voculabary

print(len(text3))    # without counting spaces (so har space par tod deta hai , so har 2 spaces kay beech wala jitna bhi content hai - that is a token / elemnt)
print(len(set(text3)))

# sorted (set(texxt3))    # this will show all the unique words.

# calculating lexical richness
# --------------------------------

# trying to see ki -> total words may say kitne words duplicate hai. and foudn the richness of the text.
# total is 44k , unique are 3k this means 41k words are duplicate.
# so the richness is very low .

len(set(text3)) / len(text3) *100

# -------------------------------------------------

# the average number of times a single word repeats
len(text3) / len(set(text3))          #  total/uinique = is kitne bar ek word repeat hua.

# --------------------------------------------------

# the average number of times the letter 'a' repeats itself
text3.count('a') / len(text3) * 100

# 'a' in text4
text4.count('a') / len(text4) * 100

def lexical_diversity(text):
  return len(set(text)) / len(text) * 100

def percentage(count, total):
  return 100 * count / total

lexical_diversity(text3)
lexical_diversity(text4)

percentage(text3.count('a'), len(text3))
percentage(text4.count('a'), len(text4))

print(sent1)
# ['Call', 'me', 'Ishmael', '.']

lexical_diversity(sent1)
# 100.0 percentage - this means all the words which are there in sent1 are unique and nothing is duplicate .

print(sent2)
# ['The', 'family', 'of', 'Dashwood', 'had', 'long', 'been', 'settled', 'in', 'Sussex', '.']

lexical_diversity(sent2)
# 100.0 percentage - this means all the words which are there in sent2 are unique and nothing is duplicate .

# python
# -----------------------------
# list + list = is a list.

['monty', 'python'] + ['and', 'the', 'fun']
# ['monty', 'python', 'and', 'the', 'fun']

#-----------------------------------------
# Let's define some example lists (since sent1, sent3, sent4, text4, text5 weren't defined)

sent1 = ['In', 'the', 'beginning']
sent3 = ['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth', '.']
sent4 = ['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', 'and', 'of', 'the', 'House', 'of', 'Representatives', ':']

#-----------------------------------------
print(sent3 + sent4)
# ['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth', '.',
#  'Fellow', '-', 'Citizens', 'of', 'the', 'Senate', 'and', 'of', 'the', 'House',
#  'of', 'Representatives', ':']

#--------------------------------
# we can also do append

sent1.append("some")
print(sent1)
# ['In', 'the', 'beginning', 'some']

#--------------------------------
# create sample text lists (imitating nltk.text.Text converted to list)
text4 = ['And', 'the', 'earth', 'was', 'without', 'form', 'and', 'void', 'and', 'darkness', 'was', 'upon', 'the', 'face', 'of', 'the', 'deep']
text5 = ['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth']

#--------------------------------
# access and index
print(text4[3])         # gives the element at index 3
print(text4.index('and'))  # gives the first index position of 'and'

#--------------------------------
# indexing [start:stop:step]
print(text5[1:3:2])  # every 2nd element between 1 and 3
print(text5[:])       # all elements
print(text5[-1:])     # last element
print(text5[1:2])     # element at index 1 aka 2nd word

#--------------------------------
# list mutability examples (convert to list if it was nltk.Text)
# since we defined them as lists, it's already okay
text5[1:2] = ['heelo']  # replacing one element with another
print(text5)

text5[2:4] = ['hi', 'hello', 'how']  # replacing 2 elements with 3 new ones
print(text5)

text4[3:4] = ['hello', 'hi', 'how']  # replacing 1 element with 3
print(text4)

# ---------------------------------

# frequency distribution      ( know more in this )
# -------------------------------------------------------

fdist1 = FreqDist(text1)      # shows the number of times each words occurs
fdist1
# ',': 18713, 'the': 13721, '.': 6862, 'of': 6536, 'and': 6024 ...

fdist1.most_common(50)   # top 50 wala deta hai

fdist1['whale']

# a token is a ---
# " hello my name is ujwal "
# so tokes are ->  ['hello','my','name','is','ujwal']   # space ko chod kay baki sab jo words ya symbols are tokens. # split_by_space.

# fine-grained selection of words
# -----------------------------------

# a. {w | w ε V & P(w)}          # V means - Voculabouary (all unique words)   # ε means - belong (epselone)
# the set of all words
# words belong to the vocabulary
# and word has the property

# b. [w for w in V if p(w)]

V = set(text1)

long_words = [w for w in V if len(w) > 15]  # list comphrehension
# so here - we loop into V and see if that word is 15 char long, and then if yes then we return that w .
#  and fir vo word will be appended in the long_word list.  and it iterates and checks and soo onn.

print(long_words)

# its basically :-

# word = [] # making a empty list

# for w in V:
#   if len(w) > 15:
#     word = word + [w]   # we can also simply append in the word list.

# print(word)


#----------------------------------------------------------

# now we sort the words

sorted(long_words)

fdist5 = FreqDist(text5)       # frequency distribution .
fdist5

# give me the list of words whose length is greater than 7 and occur for more than 7 times

fdist5_len_fd7 = [w for w in set(text5) if len(w) > 7 and fdist5[w] > 7]
sorted(fdist5_len_fd7)

# other method think -
#but there must be a method of the set from which we can get the number of appreances
#there must be a method in the list . and i can use the list method. after converting set to a list

# collocation and bigrams
#-----------------------------
list(bigrams(['more','is','said','than','done']))

from nltk.book import *
text4.collocations()        # collocations are words which are used together . United states , strong coffee , heavy rain , nice shirt

# three letter words
# how many times is the three letter words occuring in the given text
# and so on

len_words = [len(w) for w in text1]
# len_word = set(len_words)

fdist = FreqDist(len_words)    # not good, since set nahi lagana that shyad. (since abb it dosent have the count)
print(fdist)

print(fdist[3])

# most commonly used words and the number of times they occur
fdist.most_common()

# the frequency of a 3 letter word in a given text
fdist.freq(3)

#total number of tokens in text 1
len(text1)

# Example                                           Description

# vfdist = FreqDist(samples)                        create a frequency distribution containing the given samples
# fdist[sample] += 1                                increment the count for this sample
# fdist['monstrous']                                count of the number of times a given sample occurred
# fdist.freq('monstrous')                           frequency of a given sample
# fdist.N()                                         total number of samples
# fdist.most_common(n)                              the n most common samples and their frequencies
# for sample in fdist:                              iterate over the samples
# fdist.max()                                       sample with the greatest count
# fdist.tabulate()                                  tabulate the frequency distribution
# fdist.plot()                                      graphical plot of the frequency distribution
# fdist.plot(cumulative=True)                       cumulative plot of the frequency distribution
# fdist1 |= fdist2                                  update fdist1 with counts from fdist2
# fdist1 < fdist2                                   test if samples in fdist1 occur less frequently than in fdist2

# # .... ek ek run kar and see ...

# fdist = FreqDist(text1)
# print(fdist)

# fdist[text2] += 1

# fdist['monstrous']

# fdist.freq('monstrous')

# fdist.N()

# fdist.most_common(50)

# for sample in fdist:
#   print(sample)

# fdist.max()

# fdist.tabulate()

# fdist.plot()

# fdist.plot(cumulative=True)

# fdist1 |= fdist2

# vfdist = FreqDist(samples)
# create a frequency distribution containing the given samples

# frequency distribution is basically - the count of each word or element. --------------------------

fdist = FreqDist(text1)
fdist

# fdist[sample] += 1

fdist['whale']

# increment the count for this sample
fdist['whale'] += 1
fdist['whale']

# fdist['monstrous']
# count of the number of times a given sample occurred

fdist['monstrous']

# fdist.freq('monstrous')
# frequency of a given sample

fdist.freq('monstrous')

# fdist.N()
# total number of samples

fdist.N()

# fdist.most_common(n)
# the n most common samples and their frequencies

fdist.most_common(50)

# for sample in fdist:
# iterate over the samples

for w in fdist:
    print(w)

# fdist.max()
# sample with the greatest count

fdist.max()

# fdist.tabulate()
# tabulate the frequency distribution

fdist.tabulate()

# fdist.plot()
# graphical plot of the frequency distribution

fdist.plot()

# fdist.plot(cumulative=True)
# cumulative plot of the frequency distribution

fdist.plot(cumulative=True)

# fdist1 |= fdist2
# update fdist1 with counts from fdist2

fdist1 = FreqDist(text1)
fdist2 = FreqDist(text2)

fdist1 |= fdist2
fdist1

# fdist1 < fdist2
# test if samples in fdist1 occur less frequently than in fdist2

fdist1 < fdist2

# -----------------------------done ---------------------

# give me the frequency of the word that is used most often within 'text1'
# frequency percentage is ->  vo word ka count / total word count.
# -------------------------------------------------------------------------

fdist1 = FreqDist(text1)
top50 = fdist1.most_common(50)
comma_ka_freq_percentage = top50[0][1] / len(text1)   # top[0][1] -> matlab comma ',' ka frequency
comma_ka_freq_percentage

# ------------------we can also do ------------------------

fdist= FreqDist(text1)

fdist[fdist.max()]

fdist.N()

fdist[fdist.max()]/ fdist.N() * 100

fdist.freq(fdist.max()) * 100

# give me the list of words that ends with 'ableness'
# also find out the frequency distribution of all the words received
# --------------------------------------------------------------------

vocabulary  = set(text1)
ableness_words = [word for word in vocabulary if word.endswith('ableness')]
ableness_words

sorted_words = sorted(ableness_words)
sorted_words

len_sorted_words = len(sorted_words)
len_sorted_words

fdist = FreqDist(text1)
fdist

fdist_sorted_words = [(word, fdist[word]) for word in sorted_words]
fdist_sorted_words

# abb sabka 'ableness' word ka ek ek nahi but sath may freq distribution nikal hai then -
# we can manually count - 10

# indexes = index(sorted_words)     # find the proper code
# sum = fdist[indexes] .......

# find out all the words that are starting with a capital letter
# check out the frequency distribution of each of these words

vocabulary  = set(text6)
Title_words = [word for word in vocabulary if word.istitle()]
Title_words

# sorted_words = sorted(Title_words)
# sorted_words

# len_sorted_words = len(sorted_words)
# len_sorted_words

fdist = FreqDist(text6)       # this is imp to be done ...  know more ....
fdist

fdist_title_words = [(word, fdist[word]) for word in Title_words]
sorted(fdist_title_words)

# .



# # corpus



# .

# corpus -  means some large amount of text.
# --------------------------------------------------------------------

nltk.corpus.gutenberg.fileids()

emma = nltk.corpus.gutenberg.words('austen-emma.txt')
len(emma)
emma

list(emma)

emma = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))
emma.concordance("surprize")
# know more about corpus and concordance

# calculate the staistics for
# average word length
# average sentence length
# number of times each vocabulary item appears in the text (lexical diversity score)
# ------------------------------------------------------------------------------
nltk.download('punkt_tab')
for fileid in gutenberg.fileids():
  num_chars = len(gutenberg.raw(fileid))
  num_words = len(gutenberg.words(fileid))
  num_sents = len(gutenberg.sents(fileid))
  num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))
  print(round(num_chars / num_words), round(num_words / num_sents), round(num_words / num_vocab),fileid)

# calculate the sentences in shakespeare-macbeth

macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')
macbeth_sentences

# for sentence in macbeth_sentences:
#     print(sentence)

len(macbeth_sentences)

# find out the length of the longest sentence and
# find out the longest sentence
# ----------------------------------------
length_longest_sentences = [len(sent) for sent in macbeth_sentences]
length_longest_sentence = max(length_longest_sentences)
length_longest_sentence

longest_sentence = [sent for sent in macbeth_sentences if macbeth_sentences if len(sent) == length_longest_sentence]
longest_sentence

# web and chat text
# ---------------------------------

from nltk.corpus import webtext
for fileid in webtext.fileis():
  print(fileid, webtext.raw(fileid)[:65], '...')

from nltk.corpus import nps_chat
chatroom = nps_chat.posts('10-19-20s_706posts.xml')
chatroom

for chat in chatroom:
  print(chat)

chatroom[100]

# brown corpus
# --------------------------
from nltk.corpus import brown
brown.categories()

print(list(brown.words()))
len(brown.words())

# list(brown.words(categories='news'))
# len(brown.words(categories='news'))

# # for ind, ele in enumerate(brown.words()):
# #   print(ind, ele)

# # for ind, ele in enumerate(brown.words(categories='news')):
# #   print(ind, ele)

brown.words(fileids=['cg22'])

# genres: categories

brown.sents(categories=['news', 'editorial', 'reviews'])
len(brown.sents(categories=['news', 'editorial', 'reviews']))


brown.sents(categories=['news', 'editorial'])
len(brown.sents(categories=['news', 'editorial']))

brown.sents(categories=['news'])
len(brown.sents(categories=['news']))

from nltk.corpus import brown
news_text = brown.words(categories='news')

# list(news_text)
# len(news_text)

fdist = nltk.FreqDist(w.lower() for w in news_text)
fdist

modals = ['can', 'could', 'may', 'might', 'must', 'will']
for m in modals:
    print(m + ':', fdist[m], end=' ')

# what, when, where, who, and why
# categories: new, editorial, reviews
# -------------------------------------------------------------------

from nltk.corpus import brown
news_text = brown.words(categories='news')


fdist = nltk.FreqDist(w.lower() for w in news_text)
fdist

modals = ['what', 'when', 'where', 'who', 'why']
for m in modals:
    print(m + ':', fdist[m], end=' ')

print()

from nltk.corpus import brown
news_text = brown.words(categories='editorial')



fdist = nltk.FreqDist(w.lower() for w in news_text)
fdist

modals = ['what', 'when', 'where', 'who', 'why']
for m in modals:
    print(m + ':', fdist[m], end=' ')



print()

from nltk.corpus import brown
news_text = brown.words(categories='reviews')

fdist = nltk.FreqDist(w.lower() for w in news_text)
fdist

modals = ['what', 'when', 'where', 'who', 'why']
for m in modals:
    print(m + ':', fdist[m], end=' ')

# conditional frequency distribution
# -------------------------------------------------------------------------------
cfd = nltk.ConditionalFreqDist(
    (genre, word)
    for genre in brown.categories()
    for word in brown.words(categories=genre)
)

genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']
modals_one  = ['can', 'could', 'may', 'might', 'must', 'will']
modals_two = ['what', 'when', 'where', 'who', 'why']

print()
cfd.tabulate(conditions=genres, samples=modals_one)
cfd.tabulate(conditions=genres, samples=modals_two )

# reuters corpus
# ------------------------------------------------------------------------------

from nltk.corpus import reuters


# fileids in return contain files byt the names 'text' and 'training'
var = list(reuters.fileids())


len(reuters.fileids())

reuters.categories()
len(reuters.categories())

# reuters corpus overlaps with each other
# we can ask for the topics covered by one or more documents
# or for the documents included in one or more categories
# --------------------------------------------------
reuters.categories('training/9865')
reuters.categories('training/9880')
reuters.categories(['training/9865', 'training/9880'])
reuters.categories(['training/9865', 'training/9958'])

reuters.categories('training/9958')
reuters.categories(['training/9865', 'training/9958'])

reuters.fileids('barley')
reuters.fileids(['barley', 'corn'])

cfd = nltk.ConditionalFreqDist(
    (genre, word)
    for genre in reuters.categories()
    for word in reuters.words(categories=genre)
)

genres_one = ['barley', 'wheat', 'corn', 'grain']
modals_one = ['can', 'could', 'may', 'might', 'must', 'will']
modals_two = ['what', 'when', 'where', 'who', 'why']

print()
cfd.tabulate(conditions=genres_one, samples=modals_two)

reuters.words('training/9865')[:14]
reuters.words(['training/9865', 'training/9880'])[:50]

reuters.words(categories=['barley'])
reuters.words(categories=['barley', 'corn'])

""".


<h1>Assignments</h1>




.
"""

# Assignment 1: Text Exploration

# 1. Load text2 (Sense and Sensibility) from nltk.book.
import nltk
nltk.download('gutenberg')
nltk.download('genesis')
nltk.download('inaugural')
nltk.download('nps_chat')
nltk.download('webtext')
nltk.download('treebank')
from nltk.book import *
print("Loaded text:", text2.name)

# 2. Find the concordance for the word "affection".
text2.concordance("affection")

# 3. Find words similar to "affection" in text2.
text2.similar("affection")

# 4. Find the common_contexts for the words "lady" and "gentleman" in text2.
text2.common_contexts(["lady", "gentleman"])

# 5. Generate a dispersion_plot for the words "sense", "sensibility", "emotion", and
# "reason" in text2.
text2.dispersion_plot(["sense", "sensibility", "emotion", "reason"])

# Assignment 2: Lexical Analysis

# 1. Calculate the lexical diversity of text3 (The Book of Genesis).
from nltk.book import *
lexical_diversity = len(set(text3)) / len(text3)
print("Lexical Diversity of text3:", lexical_diversity)

# 2. Calculate the percentage of times the word "God" appears in text3.
percent_god = (text3.count("God") / len(text3)) * 100
print("Percentage of 'God' in text3:", percent_god, "%")

# 3. Find the 20 most common words in text3.
from nltk import FreqDist
fdist3 = FreqDist(text3)
print(fdist3.most_common(20))

# Assignment 5: Working with Stopwords

# 1. Load the English stopwords from nltk.corpus.stopwords.
import nltk
from nltk.corpus import stopwords
from nltk.book import text1
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
print("Number of English stopwords:", len(stop_words))
print("First 10 stopwords:", list(stop_words)[:10])

# 2. Create a list of words from text1 (Moby Dick) that are not stopwords and are
# purely alphabetic.
non_stopwords = [word.lower() for word in text1 if word.isalpha() and word.lower() not
in stop_words]
print("Total non-stopwords:", len(non_stopwords))

# 3. Calculate the percentage of non-stopwords in text1.
percentage_non_stopwords = (len(non_stopwords) / len(text1)) * 100
print("Percentage of non-stopwords: {:.2f}%".format(percentage_non_stopwords))

#  Stemming

# 1. Import a stemmer (e.g., PorterStemmer) from nltk.stem.
from nltk.stem import PorterStemmer
from nltk.book import text1
from nltk.corpus import stopwords

# 2. Stem a list of words (e.g., "consultant", "consultants", "consulting",
# "consultative").
ps = PorterStemmer()
example_words = ["consultant", "consultants", "consulting", "consultative"]
print("Original Words:", example_words)
print("Stemmed Words:", [ps.stem(word) for word in example_words])

# 3. Apply the stemmer to the non-stopwords list created in Assignment 5 and display
# the first 20 stemmed words.
stop_words = set(stopwords.words('english'))
non_stopwords = [word.lower() for word in text1 if word.isalpha() and word.lower() not
in stop_words]
stemmed_words = [ps.stem(word) for word in non_stopwords]
print("\nFirst 20 Stemmed Words from text1 (Moby Dick):")
print(stemmed_words[:20])

# Parts of Speech Tagging

# 1. Import the pos_tag function from nltk.
import nltk
from nltk import pos_tag

# 2. Import a corpus for tagging (e.g., brown corpus, specifically the 'news'
# category).
from nltk.corpus import brown

# 3. Get the first sentence from the 'news' category of the brown corpus.
sentences = brown.sents(categories='news')
first_sentence = sentences[0]
print("First Sentence from 'news' category:")
print(first_sentence)

# 4. Perform parts of speech tagging on this sentence and display the results.
nltk.download('averaged_perceptron_tagger_eng')
pos_tags = pos_tag(first_sentence)
print("\nPOS Tags for the first sentence:")
print(pos_tags)

# Assignment 9: Collocations
# Load text6 (Monty Python and the Holy Grail) from nltk.book.
# Find the collocations in text6.

# 1. Load text6 (Monty Python and the Holy Grail) from nltk.book.
from nltk.book import *
print("Text 6 Title:")
print(text6)

# 2. Find the collocations in text6.
print("\nCollocations in text6:")
text6.collocations()

# Assignment 11: Basic Frequency Distribution
# Create a frequency distribution for text3 (The Book of Genesis).
# Display the 10 most common words in text3.

# 1. Create a frequency distribution for text3 (The Book of Genesis).
import nltk
from nltk.book import text3 # The Book of Genesis
from nltk import FreqDist
fdist = FreqDist(text3)

# 2. Display the 10 most common words in text3.
print("10 Most Common Words in Genesis:")
print(fdist.most_common(10))

# Counting Specific Words

# 1. Count the occurrences of the word "whale" in text1 (Moby Dick).
from nltk.book import text1, text2
from nltk import FreqDist
whale_count = text1.count("whale")
print("Occurrences of 'whale' in Moby Dick:", whale_count)

# 2. Count the occurrences of the word "the" in text2 (Sense and Sensibility).
the_count = text2.count("the")
print("Occurrences of 'the' in Sense and Sensibility:", the_count)

# Assignment 3: Working with Corpora
from nltk.corpus import gutenberg

# List all file identifiers
gutenberg.fileids()

# Load words from 'carroll-alice.txt'
alice_words = gutenberg.words('carroll-alice.txt')

# Length of Alice in Wonderland text
len(alice_words)

# Lexical diversity of the text
len(set(alice_words)) / len(alice_words)

# Assignment 4: Conditional Frequency Distribution
from nltk.corpus import brown
from nltk import ConditionalFreqDist

# Create Conditional Frequency Distribution
cfd = ConditionalFreqDist(
    (genre, word.lower())
    for genre in ['romance', 'humor', 'mystery']
    for word in brown.words(categories=genre)
    if word.lower() in ['love', 'hate', 'fear']
)

# Tabulate results
cfd.tabulate(conditions=['romance', 'humor', 'mystery'], samples=['love', 'hate', 'fear'])

# Assignment 8: Named Entity Recognition
import nltk
from nltk import ne_chunk, pos_tag, word_tokenize
from nltk.corpus import gutenberg

# Get a sentence from the Gutenberg corpus
sentence = gutenberg.raw('milton-paradise.txt')[:200]   # first 200 characters
tokens = word_tokenize(sentence)

# Perform POS tagging
tagged = pos_tag(tokens)

# Apply Named Entity Recognition
ner_tree = ne_chunk(tagged)

# Display the result
print(ner_tree)
ner_tree.draw()   # (optional) opens a tree window for visualization

# Assignment 8: Named Entity Recognition

import nltk
from nltk import ne_chunk, pos_tag, word_tokenize
from nltk.corpus import gutenberg

# Get a sample sentence from the Gutenberg corpus
sentence = gutenberg.raw('milton-paradise.txt')[:200]

# Tokenize the sentence
tokens = word_tokenize(sentence)

# Perform Parts of Speech tagging
tagged = pos_tag(tokens)

# Apply Named Entity Recognition
ner_tree = ne_chunk(tagged)

# Display the result
print(ner_tree)
ner_tree.draw()   # optional: visual tree representation

# Assignment 10: More Conditional Frequency Distribution

from nltk.corpus import inaugural
from nltk import ConditionalFreqDist

# Create Conditional Frequency Distribution
cfd = ConditionalFreqDist(
    (fileid, word.lower())
    for fileid in inaugural.fileids()
    for word in inaugural.words(fileid)
    if word.lower() in ['great', 'america', 'world']
)

# Tabulate the results
cfd.tabulate(samples=['great', 'america', 'world'])

# Assignment 13: Finding the Length of a Text

from nltk.book import text4

# Total number of words
total_words = len(text4)

# Total number of unique words
unique_words = len(set(text4))

print("Total number of words:", total_words)
print("Total number of unique words:", unique_words)

# Assignment 14: Simple Concordance

from nltk.book import text1

# Find the concordance for the word "monstrous"
text1.concordance("monstrous")